# ğŸš€ 14 Days AI Challenge -- Databricks Lakehouse + Apache Spark + AI & ML

This repository documents my hands-on journey through the **14 Days AI
Challenge**, focused on building an **enterprise-grade Databricks
Lakehouse architecture** for large-scale ecommerce data ingestion,
transformation, and **AI & ML analytics**.

The project demonstrates: - Real-world **data engineering workflows** -
**Unity Catalog + Volumes** based ingestion - **Bronze â†’ Silver â†’ Gold**
architecture - **Apache Spark (PySpark) fundamentals** - Preparation for
**AI, ML & Power BI integration**

------------------------------------------------------------------------

## ğŸ“Œ Project Objectives

-   Build a **Lakehouse architecture** using Databricks\
-   Ingest **large ecommerce datasets (5GB+)**\
-   Use **Unity Catalog + Volumes** for governed storage\
-   Practice **Apache Spark transformations**\
-   Prepare data for **AI, ML & BI use cases**\
-   Maintain **Git-based version control** using Databricks Repos +
    GitHub

------------------------------------------------------------------------

## ğŸ—ï¸ Architecture Overview (Lakehouse Pattern)

    Source (CSV / ZIP files)
            â†“
    Bronze Layer (Raw data in Volumes)
            â†“
    Silver Layer (Cleaned & validated data)
            â†“
    Gold Layer (Aggregations, KPIs, Features)
            â†“
    BI / AI / ML Models

<<<<<<< Updated upstream
This architecture follows **industry best practices** used in companies
like Netflix, Comcast, and Shell.

=======
>>>>>>> Stashed changes
------------------------------------------------------------------------

## ğŸ“‚ Repository Structure

    14-Days-AI-Challenge/
    â”‚
    â”œâ”€â”€ Day 1 - Databricks & Lakehouse Basics
    â”œâ”€â”€ Day 2 - Apache Spark
    â”œâ”€â”€ Day 3 - Advanced Spark Analytics
    â”œâ”€â”€ Day 4 - Delta Lake, Unity Catalog & Data Governance
    â”‚
    â”œâ”€â”€ ingestion/
    â”œâ”€â”€ bronze/
    â”œâ”€â”€ silver/
    â”œâ”€â”€ gold/
    â”œâ”€â”€ ai_ml/
    â”œâ”€â”€ schema_volume_setup/
    â””â”€â”€ README.md

------------------------------------------------------------------------

## ğŸ§° Tech Stack

-   **Databricks (Apache Spark)**
-   **Unity Catalog & Volumes**
-   **Python / PySpark**
-   **SQL**
<<<<<<< Updated upstream
=======
-   **Delta Lake**
>>>>>>> Stashed changes
-   **AI & ML (Sentiment, Classification, Embeddings -- upcoming)**
-   **GitHub (Version Control)**
-   **Power BI (Downstream consumption -- upcoming)**

------------------------------------------------------------------------

## ğŸ“Š Dataset

-   Monthly ecommerce event data (e.g., `2019-Oct.csv`, `2019-Nov.csv`)\
-   Large files (\~5GB+)\
-   Managed using:

```{=html}
<!-- -->
```
    /Volumes/workspace/ecommerce/ecommerce_data/

------------------------------------------------------------------------

# ğŸ“… Day 1 -- Databricks & Lakehouse Fundamentals

Topics: - Databricks vs Pandas/Hadoop - Lakehouse architecture (Bronze,
Silver, Gold) - Workspace structure - Unity Catalog & Volumes - Industry
use cases: Netflix, Comcast, Shell

------------------------------------------------------------------------

# ğŸ“… Day 2 -- Apache Spark Fundamentals (10th Jan 2026)

Topics: - Spark Architecture -- Driver, Executors, DAG - Lazy
evaluation - DataFrames vs RDDs - Notebook magic commands (%python,
%sql, %fs) - Reading CSV, Parquet, JSON - Core transformations - Writing
Parquet & tables

------------------------------------------------------------------------

# ğŸ“… Day 3 -- Advanced Spark Analytics (11th Jan 2026)

Topics: - Parquet reads from Volumes - Window functions (running
totals) - Funnel analysis (view â†’ cart â†’ purchase) - Conversion without
pivot - Catalyst & lazy evaluation deep dive - Databricks chat
discussions on DAGs, shuffles, execution planning

------------------------------------------------------------------------

# ğŸ“… Day 4 -- Delta Lake, Unity Catalog & Data Governance (12th Jan 2026)

Topics: - saveAsTable and managed Delta tables - Unity Catalog governed
storage - DESCRIBE DETAIL for physical location - CTAS (Create Table As
Select) pattern - Schema enforcement testing - Schema evolution using
mergeSchema - Databricks chat deep dives on data protection & governance

------------------------------------------------------------------------

## ğŸš€ Why This Project Matters

-   Modern data engineering practices
-   Enterprise Lakehouse architecture
-   Scalable ingestion of large datasets
-   AI & ML ready pipelines
-   Production-style structure

------------------------------------------------------------------------

## ğŸ¤– AI & ML Use Cases (Upcoming)

-   Sentiment Analysis\
-   Topic Modeling\
-   Emotion detection\
-   Feature engineering\
-   BERT embeddings

------------------------------------------------------------------------

## ğŸ“Œ Next Steps (Day 5 Preview)

-   Sessionization
-   OPTIMIZE & ZORDER
-   Data quality checks
-   Feature store design

------------------------------------------------------------------------

## ğŸ‘¤ Author

<<<<<<< Updated upstream
**Venkat M**
Data Engineering \| Databricks \| Apache Spark \| AI & ML \| Power BI \|
Analytics Pipelines
=======
**Venkat Billa**\
Databricks \| Apache Spark \| Delta Lake \| AI & ML \| Power BI
>>>>>>> Stashed changes

GitHub: https://github.com/venkatbilla2008

------------------------------------------------------------------------

## â­ If you find this useful, feel free to star the repo!
